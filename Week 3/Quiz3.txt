1. a[2]4 == 4th neuron & 2nd layer
   a[2]12 == 12th training example & 2nd layer
   a[2] ==  2nd layer
   X is a matrix in which each column is one training example.

2. True

3. Z(l) = W(l)A(l-1)+b(l) 
   A(l) = g(l)[Z(l)]

4. sigmoid

5. (4,1)

6. Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons.

7.False

8.This will cause the inputof the tanh to also be very large, thus causing gradients to be close to zero.The optimization algorithm will thus become slow.

9.W(1) will have shape (4,2)
  b(1) will have shape (4,1)
  W(2) will have shape (1,4)
  b(2) will have shape (1,1)

10. Z and A are (4,1)