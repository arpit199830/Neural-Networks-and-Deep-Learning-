1. We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.

2. learning rate alpha,
   size of hidden layer n(l),
   number of iterations,
   number of layers L in neural network

3. The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.

4. False

5. for(i in range(1, len(layer_dims))):
   parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01
   parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01

6. The number of layer is 4, hidden layer is 3.

7. True

8. True

9. W(1) will have shape(4,4)
   b(1) will have shape(4,1)
   W(2) will have shape(3,4)
   b(2) will have shape(3,1)
   W(3) will have shape(1,3)
   b(3) will have shape(1,1)

10. W(l) has shape (n(l),n(l-1))